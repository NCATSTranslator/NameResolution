# Loading NameResolution data

NameResolution data needs to be loaded as a compressed [Apache Solr](https://solr.apache.org/) database.
To create this dataset is a three-step process: synonym data generated by Babel must first be loaded into
Solr, a Solr backup created, and then compressed with a particular directory structure to be used in NameRes.

## Using the Makefile

This directory includes a Makefile that can be used to run most of these steps
automatically. This is a five-step process:

1. Edit the Makefile to choose the directory containing Babel synonym files. Note that
   all files in that directory will be used, and any files named `.txt.gz` will be uncompressed.
2. Run `make all` to download the synonym data, uncompress Gzipped files, split the larger files
   and delete the split files to avoid duplicate loading. `make all` will also start the Solr server --
   you can check this by looking for a PID file Solr in `data/solr.pid`.
3. (Optional) Access the Solr server and confirm that all the data has been loaded.
4. Run `make data/backup.done` to stop Solr, install the read-only Solr config, and create
   `data/solr-data.tar.gz` containing the complete `name_lookup/` core directory.
5. Copy `data/solr-data.tar.gz` to a web server so that it can be loaded from NameRes.

## Step-by-step instructions

1. Set up a Solr server locally. The easiest way to do this is via Docker:

   ```shell
   $ docker run -v "$PWD/data/solrdata:/var/solr/data" --name name_lookup -p 8983:8983 -t solr -p 8983 -m 12G
   ```

   You can adjust the `12G` to increase the amount of memory available to Solr. You can also add `-d` to the
   Docker arguments if you would like to run this node in the background. Note: Solr runs in standalone mode
   (no `-cloud` flag), so the data directory is `/var/solr/data/` and cores are stored directly under it.

2. Copy the synonym files into the `data/synonyms` directory. Synonym files that are too large will
   need to split it into smaller files. (`gsplit` is the GNU version of `split`, which includes support
   for adding an additional suffix to files)

   ```shell
   $ gsplit -l 5000000 -d --additional-suffix .txt SmallMolecule.txt SmallMolecule
   $ gsplit -l 5000000 -d --additional-suffix .txt MolecularMixture.txt MolecularMixture
   ```

3. Download all the synonym text files into the `data/synonyms` folder. You can download this by running `make`.

   ```shell
   $ pip install -r requirements.txt
   $ make
   ```

4. Load the JSON files into the Solr database by running:

   ```shell
   $ ./setup-and-load-solr.sh "data/synonyms/*.txt*"
   ```

   Note the double-quotes: setup-and-load-solr.sh requires a glob pattern as its first argument, not a list of files to process!

5. Stop Solr and generate the backup tarball. This stops Solr cleanly, installs the
   read-only Solr config, and tars the complete `name_lookup/` core directory (including schema
   and index data, excluding the write-ahead log):

   ```shell
   $ docker exec name_lookup solr stop -p 8983 -verbose
   $ cd data/solrdata
   $ tar zcvf ../solr-data.tar.gz --exclude='name_lookup/data/tlog' name_lookup
   ```

   The tarball contains `name_lookup/` with `conf/` (schema + config) and `data/index/`
   (Lucene index). It is fully self-contained: extract it and Solr is ready to serve queries
   with no restore step.

6. Publish `solr-data.tar.gz` to a publicly-accessible URL.

7. Use the instructions at https://github.com/helxplatform/translator-devops/tree/develop/helm/name-lookup to set up an
   instance of NameRes that downloads `solr-data.tar.gz` from this publicly-accessible URL.
   Note that the Helm chart restore step is no longer required â€” the tarball is extracted
   directly to the Solr data volume and Solr starts ready to serve queries.

The Makefile included in this directory contains targets for most of these steps.
